# ğŸ­ MoodMap: Giving LLMs the Power to See, Hear, and Feel Emotion

**MoodMap** is a **multimodal AI system** that gives large language models (LLMs) the ability to **see, hear, and feel human emotion**. By combining facial expression recognition, voice tone analysis, and natural language understanding, MoodMap creates a deeply empathetic experienceâ€”transforming static chatbots into emotionally-aware digital companions.

It senses how you're feeling in the moment, processes both your **voice and facial cues**, and responds with human-like empathy through a powerful LLM and realistic text-to-speech.

## ğŸ§  Features
- ğŸ¤ **Speech Emotion Recognition** (custom-trained model)
- ğŸ¥ **Facial Emotion Detection** using DeepFace
- ğŸ’¬ **Speech Transcription** with SpeechRecognition
- ğŸ¤– **Context-Aware GPT-4 Responses** (OpenAI)
- ğŸ—£ï¸ **Human-Like Voice Reply** (Text-to-Speech)
- ğŸ–¥ï¸ Real-time webcam feed with live emotion overlay
- ğŸ” **Fully Multithreaded** & **Asynchronous** processing

## ğŸ“¦ Project Structure
```
project/
â”œâ”€â”€ audio_data/
â”‚   â””â”€â”€ recorded_data/
â”‚       â””â”€â”€ recorded_audio.wav
â”œâ”€â”€ last_50.keras                # Trained speech emotion model
â”œâ”€â”€ last_scaler.pkl              # Feature scaler for audio features
â”œâ”€â”€ main.py                      # Main execution script
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸ› ï¸ Installation

### 1. Clone the Repository
```bash
git clone https://github.com/yourusername/moodmap.git
cd moodmap
```

### 2. Create a Virtual Environment
```bash
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

### 3. Install Dependencies
```bash
pip install -r requirements.txt
```

> Make sure you have FFmpeg installed and added to your system path for audio processing.

## ğŸ”‘ API Key
Set your OpenAI API key as an environment variable:
```bash
export OPEN_AI_API_KEY=your_openai_key_here
```

Or update the code to fetch from `os.getenv("OPEN_AI_API_KEY")`.

## ğŸš€ Running the App
```bash
python main.py
```

- Press `Q` to quit the webcam feed.
- The system records your voice, detects emotion from both video and audio, and responds with a personalized message and voice.

## ğŸ“ Tech Stack
- **OpenCV** â€“ Real-time webcam and overlay
- **DeepFace** â€“ Facial emotion recognition
- **TensorFlow / Keras** â€“ Custom LSTM model for speech emotion
- **Librosa** â€“ Audio feature extraction
- **SpeechRecognition** â€“ Voice-to-text conversion
- **OpenAI GPT-4** â€“ Chatbot intelligence
- **pyttsx3** â€“ Offline text-to-speech
- **Threading** â€“ For asynchronous video/audio/chat processes

## ğŸ—‚ Emotion Labels
Speech emotion model recognizes the following emotions:
```
['angry', 'calm', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']
```

## ğŸ¤– Model Training
The speech emotion model was trained from scratch using combined datasets:
- RAVDESS
- CREMA-D
- TESS
- Custom dataset

Audio features used:
- Zero Crossing Rate
- Root Mean Square Energy
- 13 MFCCs
- Concatenated and padded to uniform length (1620 features)

## âš ï¸ Known Issues
- Facial detection requires adequate lighting
- Voice transcription can misfire in noisy environments
- Response time may vary slightly depending on system resources

## ğŸ™Œ Acknowledgments
- [DeepFace](https://github.com/serengil/deepface)
- [Librosa](https://librosa.org/)
- [OpenAI GPT](https://platform.openai.com/)
- Emotion datasets: [RAVDESS], [CREMA-D], [TESS]

## ğŸ“œ License
MIT License

## Created by Madhumitha Kolkar 2025
